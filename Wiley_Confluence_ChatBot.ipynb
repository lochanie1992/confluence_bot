{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c9623b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models... This may take a minute.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 103/103 [00:00<00:00, 257.07it/s, Materializing param=pooler.dense.weight]                             \n",
      "BertModel LOAD REPORT from: sentence-transformers/paraphrase-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned model not found. Using pretrained T5 model...\n",
      "To use a fine-tuned model, please run T5_QnA_FineTuning.ipynb first.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\locperera\\OneDrive - Wiley\\Documents\\AI Hackathon\\Team_Tesseract_AI_Hackcelerate\\Wiley Confluence Bot\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:130: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\locperera\\.cache\\huggingface\\hub\\models--t5-small. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Loading weights: 100%|██████████| 131/131 [00:00<00:00, 356.85it/s, Materializing param=shared.weight]                                                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Computing embeddings...\n",
      "Ready! Starting application...\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from PIL import Image, ImageTk  # Import for handling images\n",
    "import os\n",
    "\n",
    "print(\"Loading models... This may take a minute.\")\n",
    "\n",
    "# Load the fine-tuned models\n",
    "embedding_model = SentenceTransformer('paraphrase-MiniLM-L6-v2')  # Fine-tuned SentenceTransformer model\n",
    "\n",
    "# Check if fine-tuned model exists, otherwise use pretrained\n",
    "model_path = './trained_t5_model'\n",
    "if os.path.exists(model_path):\n",
    "    print(\"Loading fine-tuned T5 model...\")\n",
    "    tokenizer_t5 = T5Tokenizer.from_pretrained(model_path, local_files_only=True)\n",
    "    qa_model = T5ForConditionalGeneration.from_pretrained(model_path, local_files_only=True)\n",
    "else:\n",
    "    print(\"Fine-tuned model not found. Using pretrained T5 model...\")\n",
    "    print(\"To use a fine-tuned model, please run T5_QnA_FineTuning.ipynb first.\")\n",
    "    tokenizer_t5 = T5Tokenizer.from_pretrained('t5-small')\n",
    "    qa_model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "\n",
    "# Load the dataset from CSV (Make sure the dataset has been properly generated)\n",
    "df = pd.read_csv('confluence_pages.csv')\n",
    "\n",
    "# Preprocess the dataset: Combine title and content into a single string for embedding\n",
    "pages_data = [f\"{title}: {content}\" for title, content in zip(df['title'], df['content'])]\n",
    "\n",
    "print(\"Computing embeddings...\")\n",
    "page_embeddings = embedding_model.encode(pages_data)\n",
    "\n",
    "print(\"Ready! Starting application...\")\n",
    "\n",
    "# Initialize the QA pipeline with T5 model\n",
    "def qa_pipeline(query, context):\n",
    "    input_text = f\"question: {query} context: {context}\"\n",
    "    inputs = tokenizer_t5(input_text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    outputs = qa_model.generate(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'], max_length=150)\n",
    "    answer = tokenizer_t5.decode(outputs[0], skip_special_tokens=True)\n",
    "    return answer\n",
    "\n",
    "# Function to process user queries and return the most relevant Confluence page response\n",
    "def bot_response(query):\n",
    "    query_embedding = embedding_model.encode([query])\n",
    "    cosine_similarities = cosine_similarity(query_embedding, page_embeddings)\n",
    "    \n",
    "    # Get the index of the most relevant page based on similarity\n",
    "    most_relevant_page_index = np.argmax(cosine_similarities)\n",
    "    \n",
    "    # Get the relevant page data (title, content, etc.)\n",
    "    response = df.iloc[most_relevant_page_index].to_dict()\n",
    "    similarity_score = cosine_similarities[0][most_relevant_page_index]\n",
    "    \n",
    "    # Use the fine-tuned T5 model to get a more accurate answer based on the content\n",
    "    context = response['content']\n",
    "    answer = qa_pipeline(query=query, context=context)\n",
    "    \n",
    "    return answer, response, similarity_score\n",
    "\n",
    "# Create the main application window\n",
    "window = tk.Tk()\n",
    "window.title(\"WileyGPT - Confluence Chatbot\")\n",
    "\n",
    "# Set window size and background color\n",
    "window.geometry(\"1000x800\")\n",
    "window.config(bg=\"#F7F9FC\")  # Light background color for a modern look\n",
    "\n",
    "# Frame for the header with the welcome message\n",
    "header_frame = tk.Frame(window, bg=\"#4CAF50\", padx=20, pady=15)\n",
    "header_frame.pack(fill=\"x\", pady=20)\n",
    "\n",
    "# Welcome message\n",
    "welcome_label = tk.Label(header_frame, text=\"Welcome to WileyGPT!\", font=(\"Arial\", 22, \"bold\"), fg=\"white\", bg=\"#4CAF50\")\n",
    "welcome_label.pack(pady=5)\n",
    "\n",
    "# Load the image (assuming the image is saved as 'chatbot_image.png')\n",
    "image_path = r'C:\\Users\\locperera\\OneDrive - Wiley\\Documents\\AI Hackathon\\bot.jpg'  # Provide the correct path to the generated image\n",
    "\n",
    "# Open the image using PIL\n",
    "image = Image.open(image_path)\n",
    "\n",
    "# Resize the image if necessary to fit the UI (optional) - Fixed deprecation warning\n",
    "image = image.resize((150, 150), Image.LANCZOS)\n",
    "\n",
    "# Convert the image to a format Tkinter can handle\n",
    "photo = ImageTk.PhotoImage(image)\n",
    "\n",
    "# Create a label to display the image\n",
    "image_label = tk.Label(window, image=photo, bg=\"#F7F9FC\")\n",
    "image_label.pack(pady=10)\n",
    "\n",
    "# Instructions label (ask user to ask a question)\n",
    "prompt_label = tk.Label(window, text=\"Hi there! How can I assist you today?\",\n",
    "                        font=(\"Arial\", 14), wraplength=650, fg=\"#333333\", bg=\"#F7F9FC\")\n",
    "prompt_label.pack(pady=10)\n",
    "\n",
    "# Frame for the user input and submit button\n",
    "input_frame = tk.Frame(window, bg=\"#F7F9FC\")\n",
    "input_frame.pack(pady=15)\n",
    "\n",
    "# Entry field for user input (placed on the left side of the frame)\n",
    "user_input = tk.Entry(input_frame, font=(\"Arial\", 16), width=100, borderwidth=4, relief=\"solid\", justify=\"center\")\n",
    "user_input.pack(side=\"left\", padx=10)\n",
    "\n",
    "# Submit button (aligned to the right side of the user input)\n",
    "submit_button = tk.Button(input_frame, text=\"Submit\", font=(\"Arial\", 16), bg=\"#4CAF50\", fg=\"white\", \n",
    "                          width=15, height=2, command=lambda: on_submit(), relief=\"flat\", bd=0)\n",
    "submit_button.pack(side=\"right\")\n",
    "\n",
    "# Frame for the conversation area (initially hidden)\n",
    "conversation_frame = tk.Frame(window, bg=\"#F7F9FC\", width=900, height=500)\n",
    "conversation_frame.pack(fill=\"both\", expand=True)\n",
    "\n",
    "# Create a canvas for scrolling the conversation area\n",
    "canvas = tk.Canvas(conversation_frame, bg=\"#F7F9FC\")\n",
    "scrollbar = tk.Scrollbar(conversation_frame, orient=\"vertical\", command=canvas.yview)\n",
    "scrollable_frame = tk.Frame(canvas)\n",
    "\n",
    "# Configure the canvas to link to the scrollable frame\n",
    "canvas.configure(yscrollcommand=scrollbar.set)\n",
    "scrollbar.pack(side=\"right\", fill=\"y\")\n",
    "canvas.pack(side=\"left\", fill=\"both\", expand=True)\n",
    "\n",
    "# Create a window on the canvas to hold the content dynamically\n",
    "canvas.create_window((0, 0), window=scrollable_frame, anchor=\"nw\")\n",
    "\n",
    "# To store the displayed answers and avoid duplicates\n",
    "seen_responses = set()\n",
    "\n",
    "# Function to handle submit button click\n",
    "def on_submit():\n",
    "    query = user_input.get()  # Get user query\n",
    "    \n",
    "    if not query.strip():\n",
    "        return\n",
    "    \n",
    "    # Show \"thinking\" message\n",
    "    submit_button.config(text=\"Thinking...\", state=\"disabled\")\n",
    "    window.update()\n",
    "    \n",
    "    # Get bot response (single most relevant answer)\n",
    "    answer, response, similarity_score = bot_response(query)\n",
    "    \n",
    "    # Re-enable button\n",
    "    submit_button.config(text=\"Submit\", state=\"normal\")\n",
    "    \n",
    "    # Keep the user's question in the input box after submission\n",
    "    user_input.delete(0, tk.END)  # Clear the input box\n",
    "    user_input.insert(0, \"\")  # Reinsert the query\n",
    "\n",
    "    # Prepend the new question to the conversation (left side)\n",
    "    question_label_left = tk.Label(scrollable_frame, text=f\"User: {query}\", font=(\"Arial\", 12, \"bold\"), fg=\"#333333\", bg=\"#F7F9FC\", anchor=\"w\", justify=\"left\")\n",
    "    question_label_left.pack(fill=\"both\", padx=10, pady=5)\n",
    "\n",
    "    # Create a unique key for the response (based on title and content)\n",
    "    response_key = (response['title'], response['content'], similarity_score)\n",
    "    \n",
    "    # Check if this response has already been shown\n",
    "    if response_key not in seen_responses:\n",
    "        seen_responses.add(response_key)\n",
    "\n",
    "        # Add the response to the conversation (right side)\n",
    "        response_text_frame = tk.Frame(scrollable_frame)\n",
    "        response_text_frame.pack(fill=\"both\", expand=True, padx=10, pady=5)\n",
    "\n",
    "        # Add the response text area without a scrollbar, and expand it\n",
    "        response_text = tk.Text(response_text_frame, font=(\"Arial\", 12), width=150, height=35, wrap=\"word\", padx=15, pady=15,\n",
    "                                bg=\"#FFFFFF\", fg=\"#333333\", borderwidth=2, relief=\"solid\", state=tk.DISABLED)\n",
    "\n",
    "        # Enable text area to update the response\n",
    "        response_text.config(state=tk.NORMAL)\n",
    "        response_text.delete(1.0, tk.END)  # Clear previous response\n",
    "\n",
    "        # Insert the response in the text area with formatting\n",
    "        response_text.insert(tk.END, \"\\n\\n=====================\\n\")\n",
    "        response_text.insert(tk.END, f\"AskWiley: Great news! I've found the best match for your query. (Similarity score: {similarity_score:.4f}) Let's dive into the details:\\n\\n\")\n",
    "\n",
    "        # Apply bold formatting for the title using Tkinter tags\n",
    "        response_text.tag_configure(\"bold\", font=(\"Arial\", 12, \"bold\"))\n",
    "        response_text.insert(tk.END, f\"Title: \", \"bold\")\n",
    "        response_text.insert(tk.END, f\"{response['title']}\\n\\n\")\n",
    "        \n",
    "        # Add content and other sections with clear formatting\n",
    "        response_text.insert(tk.END, f\"Content:\\n\", \"bold\")  # Use fine-tuned model's answer\n",
    "        response_text.insert(tk.END, f\"{answer}\\n\\n\")\n",
    "        \n",
    "        # Section Formatting (bold sections for Resources, Video Links, etc.)\n",
    "        response_text.insert(tk.END, \"Resources:\\n\", \"bold\")\n",
    "        response_text.insert(tk.END, f\"{response['resources']}\\n\\n\")\n",
    "        response_text.insert(tk.END, \"Video Links:\\n\", \"bold\")\n",
    "        response_text.insert(tk.END, f\"{response['video_links']}\\n\\n\")\n",
    "#         response_text.insert(tk.END, \"Web Links:\\n\", \"bold\")\n",
    "#         response_text.insert(tk.END, f\"{response['web_links']}\\n\\n\")\n",
    "        response_text.insert(tk.END, \"Author:\\n\", \"bold\")\n",
    "        response_text.insert(tk.END, f\"{response['author']}\\n\\n\")\n",
    "        response_text.insert(tk.END, \"People Tagged:\\n\", \"bold\")\n",
    "        response_text.insert(tk.END, f\"{response['people_tagged']}\\n\\n\")\n",
    "        response_text.insert(tk.END, \"Last Modified:\\n\", \"bold\")\n",
    "        response_text.insert(tk.END, f\"{response['modified_date']}\\n\")\n",
    "        \n",
    "        response_text.insert(tk.END, \"\\n=====================\\n\")\n",
    "        response_text.config(state=tk.DISABLED)  # Disable text area again to prevent manual editing\n",
    "        \n",
    "        # Place the text box in the response frame\n",
    "        response_text.pack(fill=\"both\", expand=True)\n",
    "\n",
    "    # Update the canvas scrolling region to encompass the entire scrollable frame\n",
    "    scrollable_frame.update_idletasks()\n",
    "    canvas.config(scrollregion=canvas.bbox(\"all\"))\n",
    "\n",
    "# Run the application\n",
    "window.mainloop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
